<p style="font-size:19px; text-align:left; margin-top:    25px;"><i>German Association of Actuaries (DAV) — Working Group "Explainable Artificial Intelligence"</i></p>
<p style="font-size:25px; text-align:left; margin-bottom: 15px"><b>Model-Agnostic Explainability Methods for Regression Problems:<br>
A Case Study on Medical Costs Data</b></p>
<p style="font-size:19px; text-align:left; margin-bottom: 15px; margin-bottom: 25px">Dr. Simon Hatzesberger (<a href="mailto:simon.hatzesberger@gmail.com">simon.hatzesberger@gmail.com</a>)

<hr style="width:50%;">
<p style="font-size:16px; font-style:italic; margin-left:10%; margin-right:10%; margin-bottom: 1em;">
In this Jupyter Notebook, we offer a practical walkthrough for actuaries and data scientists on applying model-agnostic explainability methods to regression tasks, with a medical costs dataset serving as our case study.
</p>
<p style="font-size:16px; font-style:italic; margin-left:10%; margin-right:10%; margin-top: 1em; margin-bottom: 1em;">
It illuminates both global methods—such as surrogate models, PDPs, ALE plots, and permutation feature importances—for macro-level understanding of model behavior, and local methods—like SHAP, LIME, and ICE plots—for micro-level insights on individual predictions.
</p>
<p style="font-size:16px; font-style:italic; margin-left:10%; margin-right:10%; margin-top: 1em;">
The notebook provides practical code examples that readers can easily adopt, thereby providing a user-friendly introduction to using XAI methods to enhance transparency and reliability of their predictive models.
<hr style="width:50%;">
